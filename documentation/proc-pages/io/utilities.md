
# Python Utilities

The PROCESS Python utilities are located in the repository folder

```
/utilities/
```

A number of utilities for `PROCESS` are available, for instance to modify the input file `IN.DAT`, 
run `PROCESS` until a feasible solution is found, or to extract and plot data from the `PROCESS` output.

All executables use Python library functions either from the publicly available `numpy`, `scipy` 
and `matplotlib` libraries of the `PROCESS` Python libraries. To used the `PROCESS` Python libraries 
made their directory is in your Python path.

!!! Info "Python > 3"
    All Python code has been written for Python 3.

## Compare Input Files

`in_dat_comparison.py`

Tool for comparing two IN.DATs and outputting inputs in one file and not the other, inputs in both 
with different values and inputs in both with the same value.

| Argument | Description |
| ---- | --- |
| `-f` | Files to compare |
| `-s` | Save output to file called `input_comp.txt` |

## Compare MFILEs

`mfile_comparison.py`

Tool for comparing two MFILEs and outputting significant differences in numerical values.

| Argument | Description |
| ---- | --- |
| `-f` | Files to compare |
| `-s` | Save output to file called comp.txt |
| `--acc` | Percentage difference threshold for reporting |
| `--verbose` | Additional output |

## Convert MFILE to Catia CAD

`cad_output.py`

The PROCESS utility `cad_output.py` takes the `mfile.py` and produces an output file suitable for 
using in CAD programs (for testing *Catia* was used). The output file is named `PROCESS.CAD` by 
default. The output file provides a list of named parameters for input into *Catia*. Modification 
for other CAD programs may be required. The options for the script are:

```
cad_output.py [-h] [-f FILENAME] [-o OUTPUT] [-s]

Produce a CAD output file of the PROCESS MFILE file for a given scan. For info
contact james.morris2@ccfe.ac.uk

optional arguments:
-h, --help   show this help message and exit
-f FILENAME  specify input filename
-o OUTPUT    specify output filename
-s, --show   show plot as well as saving figure
```

## `plot_proc.py` to CSV

`output_data.py`

A utility to output a set of data very similar to `plot_proc.py`, but to a comma-delimited 
format for inclusion in spreadsheets. This is used by `archive.sh` to import data into the PROCESS 
runs database. For other uses, it's best to use PLOT.DAT instead, as this is always generated by 
PROCESS, and can be easily loaded into a spreadsheet.

**Input**: `MFILE.DAT` (or as specified by user)

**Output**: `process_summary.txt` (or as specified by user)

**Configuration Options**: Optional arguments are:

```
usage: output_data.py [-h] [-f FILENAME] [-o OUTPUT]

Produce a single-column comma-separated (.txt) summary for a given scan. For
info contact rich.kemp@ccfe.ac.uk or james.morris2@ccfe.ac.uk

optional arguments:
  -h, --help   show this help message and exit
  -f FILENAME  specify input filename
  -o OUTPUT    specify output filename
```

### Create csv file summarising data for database

`create_csv4database.py`

This is essentially the same tool as `output_data.py`, but the format is frozen to assure 
consistency for the PROCESS Runs database excel spreadsheet.

## Create MCNP file from `MFILE.DAT`

`mcnp_output.py`

The utility `mcnp_output.py` makes a `MFILE.DAT` and converts it to a suitable format for MCNP 
runs. The options for the script are:

```
mcnp_output.py [-h] [-f f] [-o o] [--ctf]

Process MFILE.DAT into PROCESS.MCNP file.

optional arguments:
-h, --help  show this help message and exit
-f f        File to read as MFILE.DAT
-o o        File to write as PROCESS.MCNP
--ctf       True/False flag for CTF
```

## JSON Exporter

```bash
./utilities/mfile_to_json.py
```

This script outputs the contents of the MFILE to a JSON file.

```bash
usage: mfile-to-json.py [-h] [-f filename] [-n N] [--radial_build]
                        [--vertical_build] [--all_build] [--verbose]
```

| Argument | Description |
| - | - |
| `-h, --help`    | show this help message and exit |
| `-f, [filename]` | specify MFILE file path |
| `-n, [N]` | specify scan to plot (-1=last, 0=all) |
| `--radial-build` | only output radial build |
| `--vertical-build` | only output vertical build  |
| `--all-build` | only output radial + vertical build |
| `--verbose` | output both variable name and description |

## PROCESS 2-Page Summary

> `/utilities/plot_proc.py`

A utility to produce a two-page summary of the output, including the major
parameters, poloidal and toroidal cross-sections, and temperature and density 
profiles.

```bash
python plot_proc.py [-h] [-f FILENAME] [-s]
```

If no `-f` argument is provided it assumes a file named `MFILE.DAT` is in the 
current directory.

Produces a two-page PDF file called `SUMMARY.pdf`

| Argument | Description | 
| - | - |
| `-h --help`   | show help message and exit
| `-f FILENAME` | specify input/output file prefix
| `-s, --show`  | show plot as well as saving figure

### Parameters Displayed

`runtitle` - Variable describing the purpose of the run.

`PROCESS version` - Tagged version of the `PROCESS` used for the run.

`Date` - Date of the `PROCESS` run.

`Time` - Time of the `PROCESS` run.

`User` - Name of the user who ran `PROCESS`.

`Optimisation` - Figure of merit (`minmax`) for constrained optimisation.

`Plasma Composition` - Number densities of several ion species relative to the electron density.

`Coil Currents etc` - Peak coil currents of the PF coils in $MA$, flux swing of the central solenoid 
used for startup and total available in $Wb$. Total burn time `tburn` in hrs.

`Cost of electricity` - This is the cost of electricity in $/MWh$. Check the respective cost model 
for the reference year of the inflation used.

| Geometry |
| :-------- |
| major radius $R_0$ |
| minor radius $a$ |
| aspect ratio $A$ |
| elongation at the 95% flux surface $\kappa_{95}$ |
| plasma triangularity at the 95% flux surface $\delta_{95}$ |
| plasma surface area | 
| plasma volume | 
| number of TF coils |
| inboard/outboard blanket thickness |
| inboard/outboard shield thickness |
| total fusion power |

| Power flows |
| :---------- |
| average neutron wall load $W_{all}=\frac{P_{neutrons}}{S_{plasma,surface}f_{user}}$[^2]
| normalised radius of the 'core' region $\rho_{core}$ used in the radiation correction of the 
confinement scaling[^3] [^4] | 
| the electron density at the pedestal top $n_{e,ped}[m^{-3}]$ |
| the normalised radius $\rho=r/a$ at the pedestal top |
| the helium fraction relative to the electron density | 
| the core radiation $P_{rad} (\rho<\rho_{core})$ subtracted from $P_{heat}$ in confinement scaling |
| $W_{th}$, the total radiation inside the separatrix | 
| nuclear heating power to blanket $P_{nuc,blkt}= P_{neutr} (1-e^{-\frac{\Delta x_{blkt}}{\lambda_{decay}}})$ |
| nuclear heating power to the shield $P_{nuc,shld}=P_{neutr}-P_{nuc,blkt}$ |
| power crossing the separatrix into the SoL/Divertor $P_{sep}$ | 
| L-H threshold power $P_{LH}$ | 
| divertor lifetime in years |
| high grade heat for electricity production $P_{therm}$ |
| gross cycle efficiency $P_{e,gross}/P_{therm}$ |
| net cycle efficiency $\frac{P_{e,gross}-P_{heat,pump}}{P_{therm}-P_{heat,pump}}$ | 
| net electric power $P_{e,net}=P_{e,gross}-P_{recirc}$ |
| plant efficiency $P_{e,net}/P_{fus}$ |

| Physics |
| :------ |
| plasma current $I_P[MA]$ |
| vaccuum magnetic field at in the plasma centre $B_T(R_0)$ |
| safety factor at the 95\% flux surface $q_{95}$ |
| definitions of $\beta$ as given in \cite{kovari_physics} |
| volume averaged electron temperature $\langle T_e\rangle$ and density $\langle n_e\rangle$ | 
| fraction of the line averaged electron density over the Greenwald density $\langle n_{e,line}\rangle / n_{GW}$ | 
| peaking of the electron temperature $T_{e,0}/\langle T_e\rangle$ and density $n_{e,0}/\langle n_{e,vol}\rangle$ |
| core and SoL effective charge $Z_{eff}=\sum_i f_iZ_i^2$ |
| impurity fraction $f_Z=n_Z/\langle n_e\rangle$ |
| H-factor and confinement time are calculated using a radiation corrected confinement scaling[^3] [^4]. |

| Neutral Beam Current Drive |
| :------------------------- |
| the steady state auxiliary power used for heating and current drive during the flat top phase (NOT to be confused with the start up or ramp down power requirements) |
| part of the auxiliary power that is used for heating only, but not current drive |
| current drive fractions for the inductive, auxiliary and bootstrap current |
| the neutral beam current drive efficiency $\gamma_{NB}$ |
| the neutral beam energy |
| the plasma heating used in the calculation of the confinement scaling/H-factor $P_{aux} + P_\alpha - P_{rad,core}$ |
| the divertor figure of merit $P_{sep}/R$, $P_{sep}/(\langle n_e\rangle R)$ |
| fraction of the power crossing the separatrix with respect to the LH-threshold power $P_{sep}/P_{LH}$ |
| non-radiation corrected H-factor (calculated for info only) |

## Sankey Diagram

> `./utilities/plot_sankey.py`

The power flows of the power plant will be extracted from MFILE.DAT and used to populate a
Sankey diagram. The diagram will start from the initial fusion power and show the inputs
and outputs for the power flows. The Recirculated power will finish by connecting the plasma
heating back into the fusion power.

### Usage

```
python plot_sankey.py [-h] [-e END] [-m MFILE] [-f]
```

### Output

A .pdf file is created called 'SankeyPowerFlow.pdf', and 'SankeyPowerFlow_full.pdf' if
the -f option is used, in the directory the utility was run. The full version is current
not working and will be implemented in the future.
N.B. Rounding to whole integer can cause errors of $\pm$1 between adjacent arrows.

### Example Output

<figure markdown>
![Sankey flow chart of 2018 baseline](/../images/sankey-power-flow.png){ width="100%"}
<figcaption>Figure 1: Sankey flow chart of 2018 baseline.</figcaption>
</figure>

### Options

| Argument | Description |
| - | - |
| `-h --help`       | show help message and exit      |
| `-e --end`        | file format, default = pdf      |
| `-m --mfile`      | mfile name, default = MFILE.DAT |
| `-f, --full`      | Plot a full version             |

## Sobol Method

> `./utilities/sobol_method.py`

Program to evaluate model sensistivity by Sobol's method at a given PROCESS design point. It uses 
the variance based global sensistivity analaysis to calculate the first order and total Sobol indices. 
More information on Sobol's method can be found, for example, in the testbook[1]. Note that this 
utility has a significanity longer run time that a typical evalution of PROCESS design points. This 
utilities requires the use of the Python library [SALib](https://salib.readthedocs.io/en/latest/index.html).

[1] A. Saltelli, S. Tarantola, F. Campolongo, M. Ratto, T. Andres, J. Cariboni, D. Gatelli and 
M. Saisana, (2008) "Global Sensitivity Analysis: The Primer" (New York: Wiley)

### Usage

```bash
usage: sobol_method.py [-h] [-f CONFIGFILE] [-i INPUTFILE] [-o OUTPUTVARNAME]
                       [-s SOLLIST] [-e ERRORLIST] [-c CONVLIST]
                       [-m OUTPUTMEAN] [-t ITER]
```

### Configuration File

The configuration file `sobol_method_conf.json` used the JSON format and has the following style
```
{
    "bounds": [
        [
            1.1,
            1.3
        ],
        [
            3.4,
            3.6
        ],
        [
            520000000.0,
            640000000.0
        ],
        [
            0.475,
            0.525
        ]
    ],
    "names": [
        "hfact",
        "boundl(18)",
        "alstrtf",
        "triang"
    ],
    "num_vars": 4
}
```

The file specifies a dictionary that gives all the information for running the Morris method tool. 
The number of variables considered in the Morris method with `num_vars`, the name of the variable 
as it appears the PROCESS MFILE is listed under `names` and the upper and lower bounds of the flat 
distribution is given in bounds. In addition the utility also uses `run_process.py` and therefore 
can optionally use the configuation file `run_process.conf`. Additionally, an `IN.DAT` file 
describing the relevant design point needs to be present.

### Output

This utility uses the `run_process.py` tool and therefore produces the same output files and in 
addition the tool creates several file in a .txt format. The parameter sampling points generated in 
the Sobol method sampling are saved the same folder as the program is run from as `param_values.txt`. 
The output of Sobol's method is shpwn over four files created in the same folder as the `run_process.py` 
utility working directory. Firstly, `output_solutions.txt` which contains a list of the final value of 
the figure of merit of every PROCESS run done over the calucation of the Sobol indices. Then two 
files `output_failed_solutions.txt` and `output_conv_solutions.txt` which list all PROCESS run 
solutions that failed and succeeded to converge respectively. Finally, the file `sobol.txt` which 
gives all the first order and total Sobol indices and their 95% confidence intervals.

### Options

| Argument | Description |
| - | - |
| `-h, --help`       | show this help message and exit                                          |
| `-f CONFIGFILE`    | configuration file, default = run_process.conf                           |
| `-i INPUTFILE`     | input parameters file, default = sobol_method_conf.json                  |
| `-o OUTPUTVARNAME` | PROCESS output analysed, default = capcost                               |
| `-s SOLLIST`       | filename of PROCESS outputs, default = output_solutions.txt              |
| `-e ERRORLIST`     | filename of failed PROCESS output, default = output_failed_solutions.txt |
| `-c CONVLIST`      | filename of converged PROCESS output, default = output_conv_solution.txt |
| `-m OUTPUTMEAN`    | PROCESS mean model output value, default = 8056.98 (DEMO capcost)        |
| `-t ITER`          | number of model iteration sampled, default = 100                         |

## Sobol Plotting

> `./utilities/sobol_plotting.py`

Program to plot the output of the the Sobols sensistivity analysis at a given PROCESS design point. 
It creates a bar chart showing both the first order and total Sobol indices for each variable and give 
the 95% confidence intervals.

### Usage

```bash
usage: sobol_plotting.py [-h] [-f DATAFILE] [-o OUTPUTFILE]
```

### Configuration File

The tool reads the data contained `sobol.txt` produced from the program `sobol_method.py`. The name of the 
data file can be modified using the option DATAFILE.

### Output

A .pdf file is created called `sobol_output.pdf`. The name of the produced pdf file can be specified 
using the option OUTPUTFILE.

### Options

| Argument | Description |
| - | - |
| `-h, --help`    | show this help message and exit                           |
| `-f DATAFILE`   | datafile for plotting, default = sobol.txt                |
| `-o OUTPUTFILE` | filename of outputed pdf file, default = sobol_output.pdf |

## TF Stress distribution plots

> `./utilities/plot_stress_tf.py`

Program to plot stress, strain and displacement radial distributions at the inboard mid-plane section of the TF coil.
This program uses the `SIG_TF.DAT` file, that store stress distributions of the VMCON point and stores the outputs
plots in the `SIG_TF_plots/` folder, created if not existing.

### Discussion of the stress modelling assumptions

In case of a resisitive coil, the stress is calculated from a generalized plane strain model, hence provinding vertical
stress radial distribution, alongside the radial and the toroidal ones. This is not the case for superconducting magnets
as a plane stress modelling is used for now. The reason is that a transverse orthotropic formulation of the generalized 
plane strain, is needed to correctly take the difference of the casing in the vertical direction properly. This will be
done in the near future. 

### Usage

```bash
usage: plot_stress_tf.py [-h] [-p [PLOT_SELEC]] [-sf [SAVE_FORMAT]] [-as [AXIS_FONT_SIZE]]
```

### Option

| Argument | Description |
| - | - |
| `-h, --help`    | show this help message and exit                           |
| `-f, --input-file`    | `SIG_TF.DAT` input file
| `-p, --plot_selec [PLOT_SELEC]`   | Plot selection string :                 |
| - |   - if the string contains `sig`, plot the stress distributions |
| - |   - if the string contains `strain`, plot the strain distributions |
| - |   - if the string contains `disp`, plot the radial displacement distribution |
| - |   - if the string contains `all`, plot stress and displecement distributions |
| `-sf, --save_format [SAVE_FORMAT]` | output format (default='pdf')  |
| `-as, --axis_font_size [AXIS_FONT_SIZE]` | Axis label font size selection (default=18) |

## N-Dimensional Scanner Utility

This suite of Python utilities allows the user to conduct systematic, multi-dimensional parameter 
studies with PROCESS. It systematically varies a set of N user defined parameters within predefined 
bounds. This final results can be evaluated using the corresponding visualisation tool and/or saved 
in standard NetCDF format for further analysis.

The suite contains the following executables:

* `ndscan.py` - executes the Nd-scan as specified in the configuration file.
* `ndscan_package_only.py` - creates a NetCDF output file from a previous Nd-scan run.
* `ndscan_and_package.py` - both executes the Nd-scan and creates the NetCDF output file.
* `ndscan_visualisation.py` - visualises the NetCDF output.

**Input** `ndscan.json`, `IN.DAT`

**Output**: All MFILES in subdirectory `MFILES`, packaged NetCDF output file as named in configuration 
file (default `NdscanOutput.nc`).

When running any of the ndscan tools, optional arguments are:

```
# to specify another location/name for the configfile
ndscan.py -f CONFIGFILE

Use -h or --help for help
```

For the visualisation tool the corresponding NetCDF input file can also be specified with `-f`. Per 
default `NdscanOutput.nc` is used.

**Configuration Options**: The configuration file `ndscan.json` uses the JSON format (www.json.org) 
and has the following style

```
{
    "axes": [
	{
            "lowerbound": 7.5,
            "steps": 16,
            "upperbound": 9.0,
            "varname": "rmajor"
        },
        {
            "lowerbound": 5.5,
            "steps": 16,
            "upperbound": 7.0,
            "varname": "bt"
        }
    ],
    "_comment": [
        "This field helps to describe the config file for users reading",
        "Anything you write in here will be ignored by the code",
        "Each axis has these configuration parameters available:",
        "varname",
        "lowerbound",
        "upperbound",
        "'steps': number of evaluations"
    ],
    "optionals": {
        "remove_scanvars_from_ixc": true,
        "smooth_itervars": true
    },
    "description": "Description of the goals of this specific run",
    "title": "NdscanOutput",
    "author": "Me",
    "output_vars": [
        "beta",
        "pheat",
        "powfmw",
        "pradmw",
        "powerht",
        "cirpowfr",
        "te",
        "hfact",
        "dnelimt",
        "dene",
        "rmajor",
        "bt",
        "pnetelmw",
        "coe",
        "fwbllife",
        "capcost",
        "palpmw",
        "wallmw",
        "taueff"
    ]
}
```
The only required input parameters in the configuration file are the scan axes, out of which at least 
one has to be specified. For each axis a `varname` a `lowerbound`, an `upperbound` and the number of 
`steps` > ` have to be specified. All other parameters in the configuration file are optional. The 
parameters relevant for running the N-dimensional scan are:

`_comment` Anything in the comment section (like all other undefined sections) is for the user only 
and will be ignored by the program.

`optional:remove_scanvars_from_ixc` Removes all scanning variables from the iteration variables of 
the `IN.DAT` file (default = True).

`optionals:smooth_itervars` Ensures that each next point starts from the last successful run. This 
increases the run time, but improves the convergence and reduces errors.

The parameters only relevant to the creation of the summary NetCDF file are:

`author` The author will be copied into the NetCDF file.

`description` The description will be copied into the NetCDF file.

`title` Name of the output NetCDF file (default `NdscanOutput`) that is also copied into the title 
of the NetCDF file.

`output_vars` The variables that will be extracted from the MFILEs and stored in the NetCDF file. 
(Only need when creating the NetCDF output file.)

Additional parameters can be specified as in the `config` section for the `evaluate_uncertainties.py` tool.

The resulting NetCDF file can be visualised using the `ndscan_visualisation.py` tool. It has an 
interactive menu and is fairly self-explanatory.

## Turn output into input

`write_new_in_dat.py`

This program creates a new `IN.DAT` file with the initial values of all the iteration variables 
replaced by their results in `OUT.DAT`, if that output is a feasible solution.

When a scan has been run, by default this program uses the last feasible point in that scan to write 
the new starting values. There is also an option to select the first feasible solution from a scan.

**Input**: `IN.DAT`, `MFILE.DAT`

**Output**: `new_IN.DAT`

```
usage: write_new_in_dat.py [-h] [-f MFILE.DAT] [-i IN.DAT]

optional arguments:
  -h, --help   show this help message and exit
  -lfp         use the last feasible point from a scan (default)
  -ffp         use the first feasible point from a scan
```

## Output plotting: create data file

`make_plot_dat.py`

Creates a `PLOT.DAT`-type file from `MFILE.DAT`. This is required by `plot_sweep.py`.

**Input**: `make_plot_dat.conf`, `MFILE.DAT`

**Output**: `make_plot_dat.out`

**Configuration Options**: Optional arguments are:

```
# new variables for output
make_plot_dat.py -p rmajor
# writes make_plot_dat.out in columns
make_plot_dat.py --columns
# resets make_plot_dat.conf to PLOT.DAT layout
make_plot_dat.py --reset-config
# file to read as input
make_plot_dat.py -f MFILE.DAT
# run with default parameters
make_plot_dat.py --defaults
```

An example version of `make_plo_dat.conf` might look like this:

```
# make_plot_dat.out config file.
rmajor
aspect
rminor
bt
powfmw
pnetelmw
te
pdivt
sig_tf_case
sig_tf_wp
```

## Plot scan results

`plot_mfile_sweep.py`

This utility plots normalised values of the iteration variables output by a parameter scan. Zero 
indicates an iteration variable at its lower bound and 1 an iteration variable at its upper bound.

**Input**: `MFILE.DAT`

**Output** `sweep_fig.pdf` (default of as specified by the user)

Optional arguments are:

```
# creates sweep_fig.pdf with R0, te, aspect (same variable names as in MFILE.DAT)
python plot_mfile_sweep.py -p rmajor te aspect
# creates demo1.png with Te, n
python plot_mfile_sweep.py -o demo1.png -p te dene
# creates a sweep_fig.pdf with R0, aspect with a different MFILE.DAT
python plot_mfile_sweep.py -f diff_mfile.dat -p rmajor aspect
# Show plot to screen instead of saving with R0 and aspect
python plot_mfile_sweep.py -p rmajor aspect --show

Use -h or --help for help
```

## Plot iteration variables and constraint residuals

`diagnose_process.py`

This utility aids the user to interpret PROCESS runs that do not find a feasible solution 
(unless PROCESS has terminated prematurely). It reads the `MFILE.DAT` and plots the normalised 
iteration variables, i.e. the iteration variable values normalised to their bounds such that 0 
indicates an iteration vraible at its lower bound and ` an iteration variable at its upper bound. 
Furthermore, it shows the normalised constraint residuals.

**Input**: `MFILE.DAT`

**Output**: Displays plots on screen, still need to be saved by the user! (Remember to use `-Y` 
or `-X`, if `ssh`ing into a remote machine!)

Optional arguments are:

```
# allows to specify another location/name for the MFILE
python diagnose_process.py -f MFILE.DAT

Use -h or --help for help
```

## Plot two parameters from many MFILES

`plot_comparison.py`

```
usage: plot_comparison.py [-h] [-x XAXIS] [-y YAXIS] [-e END] [f [f ...]]

Program to display the evolution of two variables in a selection of MFILEs.

positional arguments:
  f                     list of MFiles to be plotted; default = MFILE.DAT

optional arguments:
  -h, --help            show this help message and exit
  -x XAXIS, --xaxis XAXIS
                        x-axis, default=rmajor
  -y YAXIS, --yaxis YAXIS
                        y-axis, default=powfmw
  -e END, --end END     file format default = pdf
```

## Plot a pie chart of the cost breakdown

`cost_pie.py`

This utility plots the cost breakdown as a pie chart giving each component as a percentage. This allows for the most expensive areas to be easily identified. For the 1990 cost mdoel, an additional plot showing how direct , indirect and contingency costs contribute to the overall budget is shown.

**Input**: `MFILE.DAT`

**Output**: Displays plot of the cost breakdown to screen. For the 1990 cost model, the breakdown for direct, indirect and contingency are also shown. These can be saved with `-s` argument (`cost_pie.pdf` and `direct_cost_pie.pdf`).

Help information:

```
usage: cost_pie.py [-h] [-f MFILE] [-s]

Displays the cost breakdown as a pie chart. For more information contact
Stuart.Muldrew@ukaea.uk

optional arguments:
-h, --help  show this help message and exit
-f MFILE    specify the MFILE (default=MFILE.DAT)
-s, --save  save as well as displaying figure
```

## Plot a bar chart of the cost breakdown

`cost_bar.py`

This utility plots the cost breakdown as a bar chart giving the cost of each component. This allows for the most expensive areas to be easily identified. For the 1900 cost model, an additional plt showing how the direct, indirect and contingency costs contribute to the overall bidget is shown. Multiple MFILEs can be specified allowing for different PROCESS runs to be compared on the same plot. An inflation factor can be specified using the `-inf` argument, which multipled all the costs by that value.

**Input**: `MFILE.DAT`

**Output**: Displays plot of the cost breakdown to screen. For the 1990 cost model, the breakdown for direct, indirect and contingency is also shown. These can be saved with `-s` argument (`cost_bar.pdf` and `direct_cost_bar.pdf`).

Help information:

```
usage: cost_bar.py [-h] [-f f [f ...]] [-s] [-inf INF]

Displays the cost breakdown as a bar chart. Multiple MFILEs can be given and
will be plotted on the same chart. For more information contact
Stuart.Muldrew@ukaea.uk

optional arguments:
-h, --help    show this help message and exit
-f f [f ...]  specify the MFILE(s) to plot
-s, --save    save as well as displaying figure
-inf INF      Inflation Factor (multiplies costs)
```

## POPCON plot

`popcon.py`

This utility generates a POPCON plot from an MFILE that can be saved with the `-s` argument. `-x`, `-y` and `-z` allow the user to set the definition of temperature, density and power respectively that is plotted. Currently the impurity is set with an effective charge, and this can be parsed using `-zimp` (default is Argon). The routine also features a test case that can be run by supplying the `-t` argument only.

**Input**: `MFILE.DAT`

**Output**: Displays POPCON plot to screen. This can be saved with `-s` argument (`popcon.pdf`).

Help information:

```
usage: popcon.py [-h] [-f MFILE] [-s] [-t] [-x X] [-y Y] [-z Z] [-zimp ZIMP]

Displays a POPCON plot for input MFILE. For more information contact
Stuart.Muldrew@ukaea.uk or Peter.Knight@ukaea.uk

optional arguments:
-h, --help  show this help message and exit
-f MFILE    specify the MFILE (default=MFILE.DAT)
-s, --save  save as well as displaying figure
-t, --test  Test mode: ignores MFILE and runs with R Kembleton's test values
-x X        Temperature (x-axis): (0) Central (1) Volume (default=1)
-y Y        Density (y-axis): (0) Central (1) Volume (2) Line (default=1)
-z Z        Power (z-axis): (0) Aux for balance (1) Net (2) Fusion
(default=0)
-zimp ZIMP  Impurity charge (default=18 Ar)
```

## Profile plots

`plot_profiles.py`

This utility allows for plotting of the temperature and density profiles of a number of MFILEs. The options are described below:

```
    arguments:
    -h, --help              show this help message and exit
    -f MFILE [MFILE ...]    specify the llist of MFILEs to use
    -s, --save              save as well as displaying the figure
    -o,                     name of the output pdf file
    -n N,                   scan number in MFILE to use
```

## VMCON optimisation plots

`plot_opti_process.py`

Macro plotting a set of information about the `VMCON` optimisation from an output file called `OPT.DAT`. The file contains:
* The PROCESS indexes of the constraints and the variables used for the considered run.
* The variables described in [table 1](#table-1) stored for each `VMCON` iteration. Please notte that only one set of number is associated in per `VMCON` iteration.

<a name="table-1"></a>

|  Variable description  |  PROCESS code name  |  `VMCON` doc def  |
| ------------- | ------------- | ------------- |
| Normalized figure of merit | `abs(obj)` | $f(x)$ |			
| VMCON convergence criteria | `sum` | $\left| \nabla_x f(\vec{x}^{j-1})^T \cdot \vec{\delta}^{j} \right| + \sum^m_{i=1}\left| \lambda^j_i c_i(\vec{x}^{j-1}) \right|$ |
| Constraints residual quadratic sums | `sqsumsq` | $\sqrt{\sum^{m}_{i=1} c^{2}_i(\vec{x}^{j-1})}$ |
| Individual residual values | `conf(i)` | $c_i(\vec{x}^{j-1})$ |
| Normalized optimization variables values | `x(i)` | $\vec{x}^{j-1}$ |

Table 1: *Variables stored in `OPT.DAT`*

The python plot routines proposes the following plots:

1. Figure of merit plot<br>Evolution of the figure of merit with the `VMCON` index
2. Convergence plot<br>`VMCON` index evolution of:
    * The `VMCON` convergence parameter
    * The quadratic sum of the constraints residuals
    * The maximum between the `VMCON` convergence parameter and the constraints residual quadratic sum, actually used to test the PROCESS convergence.
3. Dominant constraints plots<br>The last `VMCON` iteration is used order to rank the constrints with their residual values. This allows to plot the `VMCON` index evolution of:
    * The $N_{const}^{dom}$ dominant constraints values ($N_{const}^{dom}$ can be used defined)
    * The quadratic sum of the dominant constraints
    * The total quadratic sum of the constraints<br>
The difference of the two quadratic sums allow to check of any other variables contribute to the constraints for any step of the optimisation
4. Selected constraint plot<br>Any constrints residual evolution can be plotted given its PROCESS ID number. The associated plot will contain:
    * The selected constraint evolution
    * The total quadratic sum of the constraints<br>
This allows a clearer visualisation of a given constraint evolution.
5. Major variable evolution<br>The variation amplitude of the optimisation variables $\max(\vec{x}^{j-1}) - \min(\vec{x}^{j-1})$ is used to rank the variables. This allows to plot the `VMCON` index evolution of the $N_{const}^{var}$ dominant variables ($N_{const}^{dom}$ can be used defined).
6. Selected variable pair trajectory plot<br>Any pair of variables can be selected using their PROCESS ID defined in vardes, to plot the evolution of their trajectory. The color of each points corresponds to the value of the PROCESS convergence criteris (on a base 10 lograithmic scale).

The use of the macro is described on the help option of the macro , shown for indicative purpose

```
usage: plot_opti_process.py [-h] [-p [PLOT_SELEC]] [-ndc [N_DOM_CONST]]
[-ndv [N_DOM_VAR]] [-ic [I_CONST]]
[-ixv [I_X_VAR]] [-iyv [I_Y_VAR]]
[-sf [SAVE_FORMAT]] [-as [AXIS_FONT_SIZE]]

Plot optimization information

optional arguments:
-h, --help            show this help message and exit
-p [PLOT_SELEC], --plot_selec [PLOT_SELEC]
Plot selection string :
    * If it containts 'FoM'      -> Figure of Merit plot 
    * If it containts 'conv'     -> convergence criteria plot 
    * If it containts 'domconst' -> dominant constraint plot
    * If it containts 'allconst' -> a plot for each constraint stored in All_Const/
    * If it containts 'domvar'   -> dominant variables plot 
    * If it containts 'allvar'   -> a plot for each variable in All_Var/
    * If it containts 'all'      -> all the mentionned plots (default value)
-ndc [N_DOM_CONST], --n_dom_const [N_DOM_CONST]
number of plotted dominant constaints (default=3)
-ndv [N_DOM_VAR], --n_dom_var [N_DOM_VAR]
number of plotted dominant variables  (default=4)
-ic [I_CONST], --i_const [I_CONST]
Selection of the constraint to be plotted (PROCESS number defined in vardes, default=-1)
-ixv [I_X_VAR], --i_X_var [I_X_VAR]
X variable on pair plot selection (PROCESS number defined in vardes, default=-1)
-iyv [I_Y_VAR], --i_Y_var [I_Y_VAR]
Y variable on pair plot selection (PROCESS number defined in vardes, default=-1)
-sf [SAVE_FORMAT], --save_format [SAVE_FORMAT]
output format (default='eps') 
-as [AXIS_FONT_SIZE], --axis_font_size [AXIS_FONT_SIZE]
Axis label font size selection (default=14)
```

The output can be defined in any visual data format supported by pyplot, the label font size can set. Please not that to select individual constraints or variable pair plots, it is enough to precisse the `-ic` and the `-ixv/-iyv` pair, respectively.

# Uncertainty Tools

In this section, we explain the usage of the PROCESS tools to both evaluate the uncertainties of a design point and display them using a simple plotting facility.

The uncertainty evaluation tool has a significantly longer run time than typical evaluations of PROCESS design points and therefore should only be used once a suitable design point has been found. As only user selected output data is kept, the user is recommended to put careful thought into the list of needed output variables.

## `evaluate_uncertainties.py`

This program evaluates the uncertainties of a single PROCESS design point by use of Monte Carlo method as described in[^5]. It is recommended to submit this script as a [batch job](#batch-jobs) to Freia when 1000s of sample points are required.

### Input

This script requires two files to run:

* `config_evaluate_uncertainties.json`: A configuration file which details the uncertain parameters under investigation. These are described by probability distributions such as Gaussian, lower half Gaussian, flat top, etc.

* `IN.DAT`: A PROCESS input file which describes the relevant design point. The path to this file should be specified in the `config_evaluate_uncertainties.json` file.

The configuration file `config_evaluate_uncertainties.json` uses the [JSON format](https://www.json.org), and has the following style:

```
{
    "_description": "Configuration file for uncertainties evaluation in PROCESS",
    "_author": "Process McCoder",
    "config": {
        "runtitle": "testrun for uncertainty tool",
        "IN.DAT_path": "path_to_input_file/IN.DAT",
        "working_directory": "path_to_output_folder/",
        "pseudorandom_seed": 16,
        "no_iter": 1
    },
    "uncertainties": [
        {
            "Varname": "boundu(9)",
            "Errortype": "LowerHalfGaussian",
            "Mean": 1.2,
            "Std": 0.1
        },
        {
            "Varname": "boundu(10)",
            "Errortype": "LowerHalfGaussian",
            "Mean": 1.2,
            "Std": 0.1
        },
        {
            "Varname": "coreradius",
            "Errortype": "Gaussian",
            "Mean": 0.6,
            "Std": 0.15
        }
    ],
    "output_vars": [],
    "no_scans": 1,
    "no_samples": 100,
    "output_mean": 8056.98,
    "figure_of_merit": "rmajor",
    "vary_iteration_variables": false,
    "latin_hypercube_level": 4
    ...

```
By convention, we have designated metadata about the PROCESS runs as having a preceding underscore to distinguish these values from the other configuration data used directly by the tools or PROCESS itself. Furthermore, all the optional attributes that can be changed when running PROCESS from most Python utilities, e.g. `run_process.py`, can be specified in the "config" section. All these values have default values and do not need to be set.

- `runtitle`: is a one line description of the purpose of the run to be saved in `README.txt` in the working directory as well as the `runtitle` parameter in the `OUT.DAT` and `MFILE.DAT` files. Per default it is empty.

- `IN.DAT_path:` is the name/path of the `IN.DAT` file describing the design point. If not specified it is assumed to be `IN.DAT`.

- `working_directory`: directs to the working directory in which PROCESS will be executed. It is recommended to create a directory for each run as this can aide organisation while several runs are executed with slightly different configs.

- `pseudorandom_seed`: is the value of the seed for the random number generator. It can be any integer value. If it is not specified, its default value is taken from the system clock.

- `no_iter`: sets `Niter`, the maximum number of retries that the tool will attempt if PROCESS fails to find a feasible solution. The default value is 10, but this can be changed depending on the user's preference for speed and solutions. 

- `factor`: varies the start values of the iteration variables by a `factor` of the original values. This does not change the physical meaning of the input file, but can help the solver to find a better starting point for its iteration. The default value is `factor=1.5`.

- `uncertainties`: any uncertain parameters should be specified in the `uncertainties` section. Each parameter is specified in its own sub-directory in the config file example above. For each entry, the `Varname` and `Errortype` need to be specified and each `Errorrtype` must be include the appropriate boundaries, listed below:
 - `Errortype` :
    - `Gaussian` (`Mean` and `Std`)
    - `LowerHalfGaussian` (`Mean` and `Std`)
    - `UpperHalfGaussian` (`Mean` and `Std`)
    - `Uniform` (`Lowerbound` and `Upperbound`)
    - `Relative` (`Mean` and `Percentage`)


    Please note that *all distributions are cut off at the boundaries for the input values for PROCESS*! At least one uncertain parameter has to be specified for the program to run and there is no upper limit to how many uncertain parameters can be used. However, for large numbers of uncertain parameters it is recommended to increase the number of sampling points.


- `no_samples`: sets the number of sample points in the Monte Carlo method. It is by default set to its recommended minimum value of 1000, but the user should contemplate higher values especially if a large number of uncertain parameters are involved.

- `no_scans`: can be used to set the number of scan runs in each MC sample point. Only the last scan point is stored in the data ouput. Older versions of the code made more use of this feature and it is recommended to set this to 1.

- `no_allowed_unfeasible`: is the number of allowed unfeasible points in a run which is set as 2  by default.

- `vary_iteration_variables`: This enables a shuffle of the iteration variables in the Monte Carlo method. By default it is set to false and may be set to true to recreate old runs of the MC code.

### Output 

- `uncertainties_data.h5`: This file contains the output variables of each successfully converged PROCESS run generated by the `evaluate_uncertainties.py` script. PROCESS output variables can be plotted using using the `hdf_to_scatter_plot.py` script. This file uses the [HDF format](https://www.hdfgroup.org/solutions/hdf5/) and requires [software](https://www.hdfgroup.org/downloads/hdfview/) to view its contents in a human legible format.

- `UQ_error_summary.txt`: This file is an ascii text file summarising all values of the uncertain parameter inputs, the normalised values of iteration variables (labelled "n_"variable name) and whether their runs have found a feasible solution (`ifail=1`), have encountered any process erros (`ifail=-1`, `error_status=3`) or whether they have not found a feasible solution. This file can be used to analyse parameter spaces prone to errors.

- `README.txt`, `process.log`, `MFILE.DAT`, `OUT.DAT`, `SIG_TF.DAT`, `OPT.DAT`, `PLOT.DAT`: Typical PROCESS output generated by the last run.

### Running the script

The `evaluate_uncertainties.py` script is run with with the option `-f` to specify the path to the `config_evaluate_uncertainties.json` file:

```
python3 process/uncertainties/evaluate_uncertainties.py -f process/uncertainties/config_evaluate_uncertainties.json
```
The uncertainty analysis technique used can be specified using '`-m monte_carlo/sobol_method/morris_method`' but the default is Monte Carlo. Use `-h` or `--help` for help.

## `display_uncertainties.py`

Note: The untertainties tool no longer produces an .nc file as output and this script has not been updated to reflect this change.

This is a utility to display the output file `uncertainties.nc` created by the `evaluate_uncertainties.py` tool described above.

By default, if run in the working directory of an uncertainty evaluation, it creates a scatter lot of each user defined output parameter against the next parameter in the list. It also hows the 1D histograms of each parameter distribution. If two specific variables are given as arguments, the tool plots only these two against each other.

**Input**: `uncertainties.nc`

**Output**: `Uncertainties_varname1_varname2.pdf`

Usage:

```
display_uncertainties.py [-h] [-f FILENAME] [-e END] [v [v ...]]

Program to display uncertainties of a given PROCESS design point.

positional arguments:
  v              list of variables to be plotted; default = all

optional arguments:
  -h, --help     show this help message and exit
  -f FILENAME, --filename FILENAME
                 uncertainties data file, default = uncertainties.nc
  -e END, --end END     file format default = .pdf
```

## `diagnose_uncertainties.py`

This is a python facility to display the input parameter distributions in the final runs vs. the ones specified in the input file. This can be used to determine whether the input distributions are sufficiently sampled or whether the resulting distributions are skewed due to unfeasible designs being excluded.

**Input** `uncertainties.nc`, `evaluate_uncertainties.json`

**Output** `Uncertainties_Diagnostic_varname.pdf`

Usage:

```
diagnose_uncertainties.py [-h] [-e END] [-u UNCERTAINTIES] [-f FILENAME]

Program to check the final uncertainty distributions in the input parameters.

optional arguments:
  -h, --help            show this help message and exit
  -e END, --end END     file format default =pdf
  -u UNCERTAINTIES, --uncertainties UNCERTAINTIES
                        uncertainties config file default =
                        evaluate_uncertainties.json
  -f FILENAME, --filename FILENAME
                        uncertainties data file, default =uncertainties.nc
```

# Miscellaneous

## `fit_profile.py`

This is a python tool to fit a general temperature or density profile as given by the pedestalised profile parameterisation (`ipedestal=1`) to an ascii table. It is using a least squares method and it fitting the position of the pedestal as well as the peaking factors. Optional arguments are

```
  -h, --help            show this help message and exit
  -f FILENAME, --filename FILENAME
                        ascii file containing data in columns, default =
                        profile.txt
  -r RHO, --rho RHO     column of the normalised radius rho=r/a, default = 0
  -n DENSITY, --density DENSITY
                        column of the density profile, default = 1
  -t TEMPERATURE, --temperature TEMPERATURE
                        column of the temperature profile, default = 2
  -rn RHOPEDN, --rhopedn RHOPEDN
                        user defined initial guess of the density pedestal
                        position, if outside [0,1] starts at 0.9, default =
                        0.9
  -rt RHOPEDT, --rhopedt RHOPEDT
                        user defined initial guess of the temperature pedestal
                        position, if outside [0,1], starts at 0.9, default =
                        0.9
```

If the column of the density or temperature data does not exist, it is ignored. A warning is issued.

## `create_dicts.py`

This automatically generates the `process_dicts.py` file used by PROCESS utility programs. It does this by scanning the Fortran source code. The standard output should be rejected, using

`create_dicts.py > process_dicts.py`

## Line Length Checker

`line_length_standard.py`

Script to check line length of repository files

## References

[^1]: M. Kovari, R. Kemp, H. Lux, P. Knight, J. Morris, D. J. Ward *"PROCESS: a systems code for fusion power plants - Part 1: Physics"*, Fusion Engineering and Design 89, 30543069 (2014), http://dx.doi.org/10.1016/j.fusengdes.2014.09.018
[^2]: M. Kovari, F. Fox, C. Harrington, R. Kembleton, P. Knight, H. Lux, J. Morris *"PROCESS: a systems code for fusion power plants - Part 2: Engineering"*, Fus. Eng. & Des. 104, 9-20 (2016)
[^3]: H. Lux, R. Kemp, D.J. Ward, M. Sertoli *"Impurity radiation in DEMO systems modelling"*, Fus. Eng. & Des. 101, 42-51 (2015)
[^4]: H. Lux, R. Kemp, E. Fable, R. Wenninger, *"Radiation and connement in 0D fusion systems codes"*, PPCF, 58, 7, 075001 (2016)
[^5]: H. Lux, R. Kemp, R. Wenninger, W. Biel, G. Federici, W. Morris, H. Zohm, "Uncertainties in power plant design point evaluations", Fusion Engineering and Design, Vol 123, 63-66, 2017
